{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earth Engine Communicator \n",
    "API DOC: https://developers.google.com/earth-engine/api_docs#eefeatureget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "pip install earthengine-python-api??\n",
    "conda install ipykernel\n",
    "python -m ipykernel install --user\n",
    "conda install -c conda-forge geopandas\n",
    "conda install -c conda-forge ipyleaflet \n",
    "conda install -c conda-forge shapely \n",
    "conda install -c conda-forge scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import datetime\n",
    "import IPython.display\n",
    "from IPython.display import Image\n",
    "import bqplot\n",
    "import ipywidgets\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2) # for printing pretty idk what it is... print with pp.pprint(print stuff)\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import dates\n",
    "%matplotlib inline\n",
    "from shapely.geometry import shape\n",
    "import skimage\n",
    "import traitlets\n",
    "import dateutil.parser\n",
    "import numpy as np\n",
    "import ipyleaflet\n",
    "from ipyleaflet import (LayersControl, basemaps, basemap_to_tiles, LayerGroup, Map, Polygon, GeoJSON)\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Connect to Earth Engine API\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get TileLayerurl from ee to plot on ipyleaflet\n",
    "def GetTileLayerUrl(ee_image_object):\n",
    "    map_id = ee.Image(ee_image_object).getMapId()\n",
    "    tile_url_template = \"https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}\"\n",
    "    return tile_url_template.format(**map_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clc_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6d48b57abc09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#radolan_viz = GetTileLayerUrl(radolan.visualize(min=0,max=40,bands=['b1']))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclc_viz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetTileLayerUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpalette\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FFE6FF'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'FFFFA8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'FFFF00'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'E6E600'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'E68000'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'F2A64D'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'E6A600'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mclc_vector_viz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetTileLayerUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclc_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#surface_moisture_viz = GetTileLayerUrl(surface_moisture.visualize(min=0,max=28,palette=['0300ff', '418504', 'efff07', 'efff07', 'ff0303']))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrandom_points_viz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetTileLayerUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_points\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clc_vector' is not defined"
     ]
    }
   ],
   "source": [
    "#Visualization Parameters\n",
    "dark_matter_layer = basemap_to_tiles(basemaps.CartoDB.DarkMatter)\n",
    "germany_viz = GetTileLayerUrl(germany_outline.visualize())\n",
    "#radolan_viz = GetTileLayerUrl(radolan.visualize(min=0,max=40,bands=['b1']))\n",
    "clc_viz = GetTileLayerUrl(clc.visualize(min=0,max=7,palette=['FFE6FF','FFFFA8','FFFF00','E6E600','E68000','F2A64D','E6A600']))\n",
    "clc_vector_viz = GetTileLayerUrl(clc_vector.draw(color='red').visualize())\n",
    "#surface_moisture_viz = GetTileLayerUrl(surface_moisture.visualize(min=0,max=28,palette=['0300ff', '418504', 'efff07', 'efff07', 'ff0303']))\n",
    "random_points_viz = GetTileLayerUrl(random_points.draw(color='blue').visualize())\n",
    "\n",
    "# Create layer group\n",
    "layer_group = LayerGroup(layers=(ipyleaflet.TileLayer(url=clc_viz,name='Corine Land Cover 2012'), \n",
    "                                 ipyleaflet.TileLayer(url=germany_viz, name='german boundary'), \n",
    "                                 ipyleaflet.TileLayer(url=clc_vector_viz, name='clc_vector'),\n",
    "                                 ipyleaflet.TileLayer(url=random_points_viz, name='rand point')\n",
    "                                 #ipyleaflet.TileLayer(url=surface_moisture_viz, name='GSMD surface moisture')\n",
    "                                ))\n",
    "#Map options\n",
    "center,zoom = [12.727661, 51.229822],4\n",
    "\n",
    "#Interactive Visualizations\n",
    "map1 = ipyleaflet.Map(layer =dark_matter_layer, center=center, zoom=zoom, layout={'height' : '400px'})\n",
    "dc = ipyleaflet.DrawControl()\n",
    "map1.add_control(dc)\n",
    "map1.add_layer(layer_group)\n",
    "map1.add_control(LayersControl())\n",
    "map1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Coordinates from Drawn Geometry\n",
    "dc.last_draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AOI, Boundarys, Geometrys, Features, Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AOI\n",
    "ried = ee.FeatureCollection('users/tillmueller1990/ried_roi')\n",
    "ried_225_222 = ee.FeatureCollection('users/tillmueller1990/ried_225_222')\n",
    "germany = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017').filter(ee.Filter.eq('country_na','Germany'))\n",
    "\n",
    "# Create an empty image into which to paint the features, cast to byte.\n",
    "empty = ee.Image().byte();\n",
    "# Paint all the polygon edges with the same number and width, display.\n",
    "germany_outline = empty.paint(germany, 1, 3)\n",
    "\n",
    "#Sample Points Germany 500\n",
    "random_points = ee.FeatureCollection.randomPoints(clc.geometry(), 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filterparameters \n",
    "# Dates\n",
    "startTime = datetime.datetime(2009,1,1)\n",
    "endTime = datetime.datetime(2019,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precipitaion Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Radolan from Assest \n",
    "radolan = ee.ImageCollection('users/tillmueller1990/radolan/r_2009-2019')\n",
    "\n",
    "#Load GSMAP Hourly Precipitation\n",
    "gsmap = ee.ImageCollection('JAXA/GPM_L3/GSMaP/v6/operational')\n",
    "\n",
    "#CHIRPS 5km daily precipitation \n",
    "chirps  = ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil Moisture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Products provided by NASA, ESA, GEE ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NASA-USDA Global Soil Moisture Data / 3 days / 2010- 2019 / 0.25 arc degrees / SMOS lvl2 integrated into two-layer palmer model\n",
    "SMOS = ee.ImageCollection('NASA_USDA/HSL/soil_moisture')\n",
    "surface_moisture = SMOS.select('ssm') #units mm, min=0 max=25\n",
    "subsurface_moisture = SMOS.select('susm') #units: mm min=0, max=275\n",
    "moisture_profile = SMOS.select('smp') #units: fraction, min0, max 1\n",
    "\n",
    "#NASA-USDA SMAP Global Soil Moisture Data / 3 days / 2015 - 2019 / 0.25 arc degrees / SMAP level 3 + two-layer Palmer\n",
    "SMAP = ee.ImageCollection('NASA_USDA/HSL/SMAP_soil_moisture')\n",
    "\n",
    "#GLDAS-2.1: Global Land Data Assimilation System / 3 hours / 0.25 arc degrees / 2000 - 2019 / "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel 1 backscatter raw data convertet to reflectivity range between 0 and 1 indicator for high reflectivity = high water volume content and vis verse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vegetation Area (in sq.km) {'VV': -6374.363046381469}\n",
      "{ 'bands': [ { 'crs': 'EPSG:32632',\n",
      "               'crs_transform': [ 10.0,\n",
      "                                  0.0,\n",
      "                                  383818.24257877335,\n",
      "                                  0.0,\n",
      "                                  -10.0,\n",
      "                                  5605230.681458753],\n",
      "               'data_type': {'precision': 'double', 'type': 'PixelType'},\n",
      "               'dimensions': [28797, 21300],\n",
      "               'id': 'VV'}],\n",
      "  'type': 'Image'}\n"
     ]
    }
   ],
   "source": [
    "#Load Sentinel 1 and filter data\n",
    "def load_dataset(ImageCollection_ID,begin,end,aoi):\n",
    "    ic = ee.ImageCollection(ImageCollection_ID).filterDate(begin,end).filterBounds(aoi)\n",
    "    return ic\n",
    "\n",
    "def filter_sentinel1(ImageCollection,polarisation,instrumentMode,resolution):\n",
    "    ic = ImageCollection.filter(ee.Filter.listContains('transmitterReceiverPolarisation',polarisation)).filter(ee.Filter.eq('instrumentMode',instrumentMode)).filterMetadata('resolution_meters','equals', resolution)\n",
    "    return ic\n",
    "\n",
    "def seperate_look_angels(ImageCollection,polarisation):\n",
    "    Ascending = ImageCollection.filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING')).select(polarisation)\n",
    "    Descending = ImageCollection.filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING')).select(polarisation)\n",
    "    return Ascending,Descending\n",
    "\n",
    "def seperate_tiles(ImageCollection,tiles):\n",
    "    tile_list = [x]\n",
    "\n",
    "def get_geometry(ImageCollection):\n",
    "    geometry = ImageCollection.geometry().getInfo()\n",
    "    #geometry = [x for x in geometry['coordinates']]\n",
    "    return geometry\n",
    "\n",
    "def show_tiles(ImageCollection):\n",
    "    geometry = get_geometry(ImageCollection)\n",
    "    geometry_list = list(geometry['coordinates'])\n",
    "    flattened_list = [y for x in geometry_list for y in x] #De flatter list \n",
    "    unique_list = []\n",
    "    unique_list = [x for x in flattened_list if x not in unique_list]\n",
    "    print(\"different tiles: \",len(unique_list))\n",
    "    m = Map(center=(49.6252978589571, 8.34580993652344), zoom=7)\n",
    "    geo_json = GeoJSON(data=geometry, style = {'color': 'green', 'opacity':1, 'weight':1.9, 'dashArray':'9', 'fillOpacity':0.1})\n",
    "    m.add_layer(geo_json)\n",
    "    dc = ipyleaflet.DrawControl()\n",
    "    m.add_control(dc)\n",
    "    m.add_control(LayersControl())\n",
    "    return m\n",
    "    \n",
    "def get_properties(ImageCollection):\n",
    "    features = ImageCollection.getInfo()['features']\n",
    "    dict_list = []\n",
    "    for f in features:\n",
    "        prop = f['properties']\n",
    "        dict_list.append(prop)\n",
    "    df = pd.DataFrame.from_records(dict_list).drop(['system:footprint','transmitterReceiverPolarisation'],axis=1)\n",
    "    #Pandas Series of unique distinc values in df\n",
    "    unique = df.nunique()\n",
    "    im_id_list = [item.get('id') for item in ImageCollection.getInfo().get('features')]\n",
    "    date_list = [datetime.datetime.strptime(x[35:43],'%Y%m%d') for x in im_id_list]\n",
    "    #property_names = list(df.columns.values) \n",
    "    return unique, im_id_list, date_list\n",
    "\n",
    "def make_mosaic(date):\n",
    "    #Cast objects\n",
    "    #date = ee.Number(date)\n",
    "    date = ee.Date(date['value'])\n",
    "    #print(date)\n",
    "    #newList = ee.List([])\n",
    "    #print(date)\n",
    "    filterCollection = VV_Ascending.filterDate(date, date.advance(1,'day'))\n",
    "    #print(filterCollection.size().getInfo())\n",
    "    #Make the mosaic\n",
    "    image = ee.Image(filterCollection.mosaic()).copyProperties(filterCollection.first(),[\"system:time_start\"])\n",
    "    #newList.add(image)\n",
    "    #Add the mosaic to a list only if the collection has images\n",
    "    #print(image.getInfo())\n",
    "    #return ee.List(ee.Algorithms.If(filterCollection.size(), newList.add(image), newList))\n",
    "    #print(image.getInfo())\n",
    "    return image\n",
    "\n",
    "def clip_aoi(ImageCollection):\n",
    "    im = ImageCollection.clip(ried_225_222)\n",
    "    return im\n",
    "\n",
    "def plot_image(ImageCollection):\n",
    "    m = Map(center=(49.6252978589571, 8.34580993652344), zoom=7)\n",
    "    ic = GetTileLayerUrl(ImageCollection.first().visualize())\n",
    "    m.add_layer(ic)\n",
    "    dc = ipyleaflet.DrawControl()\n",
    "    m.add_control(dc)\n",
    "    m.add_control(LayersControl())\n",
    "    return m\n",
    "\n",
    "#Time of interest\n",
    "begin = ee.Date.fromYMD(2015,1,1)\n",
    "end = ee.Date.fromYMD(2016,1,1)\n",
    "date_range = end.difference(begin, 'day')\n",
    "#Source dataset\n",
    "sentinel1 = load_dataset('COPERNICUS/S1_GRD',begin,end,ried_225_222)\n",
    "#Filter dataset for High resolution and Vertical transmitt vertical receive\n",
    "sentinel1_VV = filter_sentinel1(sentinel1,'VV','IW',10)\n",
    "#Filter for different look angles\n",
    "VV_Ascending,VV_Descending = seperate_look_angels(sentinel1_VV,'VV')\n",
    "\n",
    "#Get geometry for plotting different tiles\n",
    "#geometry_vv = get_geometry(VV_Ascending)\n",
    " #Plot Tiles\n",
    "\n",
    "#Get list of ids,dates and unique count of prop\n",
    "unique, im_id_list, date_list = get_properties(VV_Ascending)\n",
    "date_list = ee.List([ee.Date(x) for x in date_list])\n",
    "#pp.pprint(unique)\n",
    "VV_aoi = VV_Ascending.map(clip_aoi)\n",
    "#pp.pprint(VV_aoi.first().area().divide(1000 * 1000))\n",
    "area = VV_Descending.first().multiply(ee.Image.pixelArea()).divide(1000*1000)\n",
    "#Reducing the statistics for your study area\n",
    "stat = area.reduceRegion(ee.Reducer.sum(),ried_225_222,100)\n",
    "print('Vegetation Area (in sq.km)', stat.getInfo());\n",
    "-6000 = normal \n",
    "80 sehr wenig \n",
    "pp.pprint(area.getInfo())\n",
    "#show_tiles(VV_Ascending)\n",
    "#plot_image(VV_aoi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "newList = ee.List([])\n",
    "\n",
    "for date in date_list.getInfo():\n",
    "    mosaic = ee.ImageCollection(ee.List(make_mosaic(date)))\n",
    "    #print(mosaic.getInfo())\n",
    "    #print(mosaic)\n",
    "    #print(newList)\n",
    "newcol = ee.ImageCollection(newList)\n",
    "#newcol = ee.ImageCollection(ee.List(date_list.iterate(make_mosaic(date), ee.List([]))))\n",
    "#pp.pprint(newcol.getInfo())\n",
    "\n",
    "#pp.pprint(newcol.getInfo()['features'])\n",
    "\n",
    "#VV_Ascending_mosaic = make_mosaic(date_list)\n",
    "#Iterate over the range to make a new list, and then cast the list to an imagecollection\n",
    "#newcol = ee.ImageCollection([x]\n",
    "\n",
    "#print(unique, im_id_list, date_list)\n",
    "#pp.pprint(sentinel1_properties)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'ImageCollection', 'bands': [], 'features': []}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(newcol.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e395f3f502664f3bbcce64a8d60002f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(basemap={'url': 'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', 'max_zoom': 19, 'attribution': 'Map â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Visualization Parameters\n",
    "dark_matter_layer = basemap_to_tiles(basemaps.CartoDB.DarkMatter)\n",
    "germany_viz = GetTileLayerUrl(germany_outline.visualize())\n",
    "#radolan_viz = GetTileLayerUrl(radolan.visualize(min=0,max=40,bands=['b1']))\n",
    "#clc_viz = GetTileLayerUrl(clc.visualize(min=0,max=7,palette=['FFE6FF','FFFFA8','FFFF00','E6E600','E68000','F2A64D','E6A600']))\n",
    "#clc_vector_viz = GetTileLayerUrl(clc_vector.draw(color='red').visualize())\n",
    "#surface_moisture_viz = GetTileLayerUrl(surface_moisture.visualize(min=0,max=28,palette=['0300ff', '418504', 'efff07', 'efff07', 'ff0303']))\n",
    "#random_points_viz = GetTileLayerUrl(random_points.draw(color='blue').visualize())\n",
    "sentinel1_viz = GetTileLayerUrl(VV_aoi.first().visualize())\n",
    "\n",
    "# Create layer group\n",
    "layer_group = LayerGroup(layers=(#ipyleaflet.TileLayer(url=clc_viz,name='Corine Land Cover 2012'), \n",
    "                                 ipyleaflet.TileLayer(url=germany_viz, name='german boundary'),\n",
    "                                 #ipyleaflet.TileLayer(url=clc_vector_viz, name='clc_vector'),\n",
    "                                 #ipyleaflet.TileLayer(url=random_points_viz, name='rand point')\n",
    "                                 #ipyleaflet.TileLayer(url=surface_moisture_viz, name='GSMD surface moisture')\n",
    "                                 ipyleaflet.TileLayer(url=sentinel1_viz, name='Sentinel 1 VV ')\n",
    "                                ))\n",
    "#Map options\n",
    "center,zoom = (49.6252978589571, 8.34580993652344),6\n",
    "\n",
    "#Interactive Visualizations\n",
    "map1 = ipyleaflet.Map(layer= basemap_to_tiles(basemaps.CartoDB.DarkMatter), center=center, zoom=zoom, layout={'height' : '400px'})\n",
    "dc = ipyleaflet.DrawControl()\n",
    "map1.add_control(dc)\n",
    "map1.add_layer(layer_group)\n",
    "map1.add_control(LayersControl())\n",
    "map1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corine Land Cover "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load last corine land cover image \n",
    "clc = ee.Image.load('COPERNICUS/CORINE/V18_5_1/100m/2012', -1)\n",
    "\n",
    "# Mask areas where soil moisture measurements valid (farmland cat.:11-17)\n",
    "clc_mask = clc.gte(11)and(clc.lte(17)) #binary map for updateMask\n",
    "clc = clc.updateMask(clc_mask) #set mask for not Farmland\n",
    "\n",
    "# Clip to extend of germany\n",
    "clc = clc.clip(germany) \n",
    "\n",
    "# Feature Collection of clc farmland cat.: 11-17 with 350m resolution\n",
    "clc_vector = clc.reduceToVectors(geometry=germany, crs=clc.projection(), scale=350, geometryType='polygon', eightConnected = False)\n",
    "#Limit to first 5000 Features \n",
    "clc_vector = clc_vector.limit(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLDAS-2.1: Global Land Data Assimilation System \n",
    "Global Land Data Assimilation System (GLDAS) ingests satellite and ground-based observational data products. Using advanced land surface modeling and data assimilation techniques, it generates optimal fields of land surface states and fluxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLDAS = ee.ImageCollection('NASA/GLDAS/V021/NOAH/G025/T3H')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Hessisches Ried \n",
    "Only year by year otherwise memory exceeded\n",
    "### SMOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2019,1,1),datetime.datetime(2020,1,1)\n",
    "aoi = ried\n",
    "sm = SMOS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi, scale=1000)\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    data_aoi['geometry'] = aoi.getInfo()['features']\n",
    "    data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['properties']['ID'][23:29],'%y%m%d')\n",
    "    df = pd.DataFrame(data_aoi)\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "#### Iterate over all impages\n",
    "#df_all = pd.DataFrame({})\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "df_all.to_csv(\"SMOS_ried_2019_2020.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chrisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2019,1,1),datetime.datetime(2020,1,1)\n",
    "aoi = ried\n",
    "sm = chirps.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    data_aoi['geometry'] = aoi.getInfo()['features']\n",
    "    data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][22:31],'%Y%m%d')\n",
    "    df = pd.DataFrame(data_aoi)\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "df_all.to_csv(\"chirps_ried_2019_2020.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLDAS 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2010,1,1),datetime.datetime(2010,6,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2010_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2010,6,1),datetime.datetime(2011,1,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2010_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2011,1,1),datetime.datetime(2011,6,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2011_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2011,6,1),datetime.datetime(2012,1,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2011_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2012,1,1),datetime.datetime(2012,6,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2012_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2012,6,1),datetime.datetime(2013,1,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2012_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2013,1,1),datetime.datetime(2013,6,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2013_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2013,6,1),datetime.datetime(2014,1,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2013_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2014,1,1),datetime.datetime(2014,6,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2014_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2014,6,1),datetime.datetime(2015,1,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2014_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2015,1,1),datetime.datetime(2015,6,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2015_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2015,6,1),datetime.datetime(2016,1,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2015_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2016,1,1),datetime.datetime(2016,6,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2016_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2016,6,1),datetime.datetime(2017,1,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2016_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2017,1,1),datetime.datetime(2017,6,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2017_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2017,6,1),datetime.datetime(2018,1,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2017_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2018,1,1),datetime.datetime(2018,6,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2018_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2018,6,1),datetime.datetime(2019,1,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2018_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions, Source Dataset, TOI, \n",
    "start,end = datetime.datetime(2019,1,1),datetime.datetime(2019,6,1)\n",
    "aoi = ried\n",
    "sm = GLDAS.filterDate(start,end)\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, aoi):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegion(ee.Reducer.mean(), aoi) #,scale=1000\n",
    "    # Convert to Pandas Dataframe\n",
    "    data_aoi = fc_image_red.getInfo()\n",
    "    #data_aoi['date'] = datetime.datetime.strptime(image.getInfo()['id'][31:39],'%Y%m%d')\n",
    "    #df = pd.DataFrame(data_aoi,columns=['Albedo_inst', 'AvgSurfT_inst', 'CanopInt_inst', 'ECanop_tavg', 'ESoil_tavg', 'Evap_tavg', 'LWdown_f_tavg', 'Lwnet_tavg', 'PotEvap_tavg', 'Psurf_f_inst', 'Qair_f_inst', 'Qg_tavg', 'Qh_tavg', 'Qle_tavg', 'Qs_acc', 'Qsb_acc', 'Qsm_acc', 'Rainf_f_tavg', 'Rainf_tavg', 'RootMoist_inst', 'SWE_inst', 'SWdown_f_tavg', 'SnowDepth_inst', 'Snowf_tavg', 'SoilMoi0_10cm_inst', 'SoilMoi100_200cm_inst', 'SoilMoi10_40cm_inst', 'SoilMoi40_100cm_inst', 'SoilTMP0_10cm_inst', 'SoilTMP100_200cm_inst', 'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'Swnet_tavg', 'Tair_f_inst', 'Tveg_tavg', 'Wind_f_inst'])\n",
    "    df = pd.DataFrame.from_records([data_aoi])\n",
    "    return df\n",
    "\n",
    "#List of image propertys \n",
    "im_prop = sm.first().propertyNames().getInfo()\n",
    "#Get propertie from every image to a List\n",
    "im_id = [item.get('id') for item in sm.getInfo().get('features')]\n",
    "im_date = [datetime.datetime.strptime(x[31:44],'%Y%m%d_%H%M') for x in im_id]\n",
    "\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(im_id[0], aoi)\n",
    "df_all = df_all.drop([0])\n",
    "\n",
    "#### Iterate over all impages\n",
    "for i in im_id:\n",
    "    df_all = df_all.append(extract_point_values(i, aoi))\n",
    "\n",
    "#Set date and save a lot of .getInfo() calls\n",
    "df_all['date'] = im_date\n",
    "\n",
    "df_all.to_csv(\"GLDAS_ried_2019_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometrys, Regions\n",
    "points = ee.FeatureCollection([\n",
    "            ee.Feature(ee.Geometry.Point(8.234870250943459,49.80831604635797)),\n",
    "            ee.Feature(ee.Geometry.Point(8.241221721890724,49.6585087644599)),\n",
    "            ])\n",
    "point = ee.Feature(ee.Geometry.Point(8.234870250943459,49.80831604635797))\n",
    "\n",
    "#Time\n",
    "start,end = datetime.datetime(2015,1,1),datetime.datetime(2016,1,1)\n",
    "#Source Dataset\n",
    "chirps = chirps_precipitation.filterDate(start,end)\n",
    "# Function Convert a FeatureCollection into a pandas DataFrame; Features is a list of dict with the output\n",
    "def fc2df(fc):\n",
    "    # Convert a FeatureCollection into a pandas DataFrame\n",
    "    # Features is a list of dict with the output\n",
    "    features = fc.getInfo()['features']\n",
    "\n",
    "    dictarr = []\n",
    "    for f in features:\n",
    "        # Store all attributes in a dict\n",
    "        attr = f['properties']\n",
    "        # and treat geometry separately\n",
    "        attr['geometry'] = f['geometry']  # GeoJSON Feature!\n",
    "        #attr['geometrytype'] = f['geometry']['type']\n",
    "        dictarr.append(attr)\n",
    "\n",
    "    df = gpd.GeoDataFrame(dictarr)\n",
    "    # Convert GeoJSON features to shape\n",
    "    #df['geometry'] = map(lambda x: shape(x), df.geometry)    \n",
    "    return df\n",
    "\n",
    "# Function to iterate over image collection, returning a pandas dataframe\n",
    "def extract_point_values(img_id, pts):\n",
    "    image = ee.Image(img_id)\n",
    "    #Ad reducer output to the Features in the collection.\n",
    "    fc_image_red = image.reduceRegions(collection=pts, reducer=ee.Reducer.mean(), scale=1000)\n",
    "    # Convert to Pandas Dataframe\n",
    "    df_image_red = fc2df(fc_image_red)\n",
    "    # Add Date as Variable\n",
    "    df_image_red['date'] = datetime.datetime.strptime(image.getInfo()['id'][22:31],'%Y%m%d')\n",
    "    print(df_image_red)\n",
    "    return df_image_red\n",
    "\n",
    "#List of image propertys \n",
    "chirps_prop = surface_moisture.first().propertyNames().getInfo()\n",
    "print('Check 1')\n",
    "#Get propertie from every image to a List\n",
    "chirps_id = [] #empty list\n",
    "chirps_id = [item.get('id') for item in chirps.getInfo().get('features')]\n",
    "#### Create Initial Pandas Dataframe\n",
    "df_all = extract_point_values(chirps_id[0], points)\n",
    "df_all = df_all.drop([0,1])\n",
    "print(\"check 3\")\n",
    "#### Iterate over all impages\n",
    "c=0\n",
    "for i in chirps_id:\n",
    "    c = c+ 1\n",
    "    print(\"c\",c)\n",
    "    df_all = df_all.append(extract_point_values(i, points))\n",
    "print(\"check 4\")\n",
    "#### Display Results\n",
    "pp.pprint(df_all)\n",
    "rad_point1 = df_all.loc[0]\n",
    "rad_point2 = df_all.loc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.renderers.enable('notebook')\n",
    "pp.pprint(sm_point1['date'])\n",
    "rad_point1['mean'] = rad_point1['mean'].multiply(4)\n",
    "chart_sm = alt.Chart(sm_point1).mark_point().encode(x='date', y= 'smp')\n",
    "chart_pp = alt.Chart(rad_point1).mark_point().encode(x='date', y='mean')\n",
    "#alt.vconcat(chart_sm, chart_pp)\n",
    "chart_sm + chart_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(rad_point1)\n",
    "pp.pprint(sm_point1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "data = \n",
    "chart = alt.Chart(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil Moisture from Sentinel 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derived from Sentinel 1 A/B 2014-10 - Present\n",
    "s1 = ee.ImageCollection('COPERNICUS/S1_GRD')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Command Collection\n",
    ".size().getInfo() # Get collection size\n",
    ".bandNames().getInfo() # Get List with alls Bands from ee.Image not ImageCollection\n",
    ".geometry().bounds().getInfo() # get Geometry of a Feature // Get bounding box of this geometry\n",
    ".limit(5000) # Limit to the first 5000 Elements/Features\n",
    "\n",
    "#Vectorizing\n",
    "var vectors = image.reduceToVectors({\n",
    "  geometry: FeatureCollection,\n",
    "  crs: image.projection(),\n",
    "  scale: 1000,\n",
    "  geometryType: 'polygon',\n",
    "  eightConnected: false,\n",
    "  labelProperty: 'zone',\n",
    "  reducer: ee.Reducer.mean()\n",
    "});\n",
    "\n",
    "#Masking\n",
    ".clip(feature)\n",
    "var image = ee.Image\n",
    "var mask = image.gte(2).and(lt(5))\n",
    "var maskedImage = image.updateMask(mask)\n",
    "\n",
    "#Visualizations\n",
    "thumbnail_url = image.getThumbUrl({\n",
    "    'bands' : '',\n",
    "    'min' : ,\n",
    "    'max' : ,\n",
    "    'region' : .geometry().bounds().getInfo() #must be a geojson \n",
    "})\n",
    "IPython.display.HTML('Thumnail URL: <a href={0}>{0}</a>'.format(thumbnail_url)) #create url to view\n",
    "IPython.display.Image(url=thumbnail_url) # view direct in notebook\n",
    "\n",
    "#Interactive Visualizations\n",
    "import ipyleaflet\n",
    "map1 = ipyleaflet.Map(zoom=3, layout={'height' : '400px'})\n",
    "dc = ipyleaflet.DrawControl()\n",
    "map1.add_control(dc)\n",
    "map1\n",
    "dc.last_draw # gives information about the last drawn polygon (coordinates etc.)\n",
    "\n",
    "#Function to create a tile layer urlfrom an gee image object\n",
    "def GetTileLayerUrl(ee_image_object):\n",
    "    map_id = ee.Image(ee_image_object).getMapId()\n",
    "    tile_url_template = \"https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}\"\n",
    "    return tile_url_template.format(**map_id)\n",
    "\n",
    "#style the image\n",
    "tile_url = GetTileLayerUrl(image.visualize(min=0, max=3000, gamma=1.5, bands=['','','']))\n",
    "map1.add_layer(ipyleaflet.TileLayer(url=tile_url))\n",
    "#or create layer groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import ee\n",
    "from ipygee import *\n",
    "\n",
    "\n",
    "def extract_time_series(start,end,coll,ried): #,sf\n",
    "\n",
    "    \n",
    "    # Obtain image collection for all images within query dates\n",
    "    coll = coll.filterDate(start,end)\n",
    "\n",
    "    # Get list of images which correspond with the above\n",
    "    images = [item.get('id') for item in coll.getInfo().get('features')]\n",
    "    store = []\n",
    "    date_store = []\n",
    "    print(images)\n",
    "    # Loop over all images and extract pixel value\n",
    "    for image in images:\n",
    "        \n",
    "        im = ee.Image(image)\n",
    "        #projection = im.projection().getInfo()['crs']\n",
    "        # Obtain date from timestamp in metadata\n",
    "        date = dt.fromtimestamp(im.get(\"system:time_start\").getInfo() / 1000.)\n",
    "        date_store.append(np.datetime64(date))\n",
    "\n",
    "        # Extract pixel value\n",
    "        data = im.reduceRegion(ee.Reducer.mean(),ried, 1000) #,1, crs=projection).get(band_name) \n",
    "        store.append(data.getInfo())\n",
    "        print(store)\n",
    "    # Scale the returned data based on scale factor\n",
    "    #store = [x * sf if isinstance(x, int) else np.nan for x in store]\n",
    "    \n",
    "    # Convert output into pandas data frame\n",
    "    df = pd.DataFrame(index=date_store, data=store, columns=['precipitation'])\n",
    "    df['store']\n",
    "    return df\n",
    "\n",
    "\n",
    "band_name = 'b1'\n",
    "coll = chirps_precipitation\n",
    "start,end = datetime.datetime(2015,1,1),datetime.datetime(2015,1,5)\n",
    "# Set up point geometry\n",
    "points = ee.FeatureCollection([\n",
    "        ee.Feature(ee.Geometry.Point(8.234870250943459,49.80831604635797)),\n",
    "        ee.Feature(ee.Geometry.Point(8.241221721890724,49.6585087644599)),\n",
    "        ])\n",
    "point = ee.Feature(ee.Geometry.Point(8.234870250943459,49.80831604635797))\n",
    "\n",
    "\n",
    "rad_data = extract_time_series(start,end,coll,ried)\n",
    "print(rad_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
