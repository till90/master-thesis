---
title: "Notes on uploading vector and raster data into Google Earth Engine (GEE)"
author: |
 | Guillermo E. Ponce-Campos
 | USDA-ARS Southwest Watershed Research Center
 | E-mail: guillermo.ponce@ars.usda.gov
 |
date: 'Updated: `r format(Sys.Date(), format="%B %d, %Y")`'
tags: [Google, Earth, Engine, R, Python, NEON, geospatial]
output: 
  html_notebook:
    theme: journal  
editor_options:    
  chunk_output_type: inline
---

This notebook is used to document the steps for uploading geospatial data into 
Google Earth Engine (GEE). These steps could change or vary depending on the 
needs of the end-user. GEE already provides an extensive catalog of public 
datasets.  This catalog keeps growing based on users' suggestions by using an 
issue tracking system called [Issue Tracker.](https://issuetracker.google.com/issues?q=componentid:184426%20is:open)
<br> 
<br>
![](images/HS_neon2017_SRER_small_1.gif) 
***Animation showing hyperspectral data visualization in GEE from the NEON 2017 Campaign at Santa Rita Experimental Range.*** Access to this in GEE browser API: http://bit.ly/hyper_srer_2017

Left panel shows plots with the spectral signature profile of the 426 bands for 
any given pixel. Other layers shown are RGB-Camera 10cm pixel size, and from
1m hyperspectral data a False color-NIR, Natural Color, and NDVI.
   
The tasks described below are performed using the server `STORM` @USDA-ARS-SWRC. 
 These are the specifications of the server as May/2018 along with the operating
system and software used:

`Server: Dell R930 (storm)`<br>
`O.S.: RedHat 7.5 (3.10.0-862.6.3.el7.x86_64)`<br>
`Python ver.: 3.64`<br>
`Google Cloud SDK ver.: 4.28` (Staging platform)<br> 
`Google Python API client library` (Python functions by Google)<br>
`Google Earthengine API` (Python functions to access GEE) <br>

### 1. Installation of Google Cloud Software Development Kit (SDK)

Google Cloud Storage is needed to serve as a `staging platform` to move data into
GEE.

You can get SDK from here: https://cloud.google.com/sdk/docs/quickstart-linux

```{bash}
# ******************** Command Line Instructions ******************************#
# Unzip gc sdk 

tar -xvf google-cloud-sdk-xxx.0.0-linux-x86_64.tar.gz    

# Install it                                                                                               
./google-cloud-sdk/install.sh          

 # Refresh environment without doing logout                                                                                                    
. ~/.bashrc          

```

### 2. Installation of `google-api-python-client` and `earthengine-api`.

This setup will enable your system to access GEE platform from 
Python (`import ee`) and also use the `earthengine` command line tool to 
move data from Google Cloud into GEE.

```{bash}
# ******************** Command Line Instructions ******************************#
# Installing Google API-Python-client library
sudo pip install google-api-python-client

# Ensure crypto library is available. No error is expected after running the following:
python -c "from oauth2client import crypt"

# If no error, continue with the Installation of Earth Engine Python API

sudo pip install earthengine-api

# Switch the expanded directory and run script
cd earthengine-api-VERSION
python setup.py install

# Authenticate into GEE from terminal. The following line should show a URL 
# that you can paste into a browser window to authenticate with your account
# that has access to GEE.
python -c "import ee; ee.Initialize()"


```
After previous steps, GEE API should be ready to use it.

### 3. Moving data from a local drive into GEE
The workflow to move data is:

**LocalDrive --> Google Cloud Storage (bucket)  --> GEE (asset)**

One of the current limitations on uploading several files into GEE is that you
must use Google Cloud Storage(GCS) as the staging platform. 
**You can also upload data using the JavaScript Code Editor**, however, this approach 
will only allow you to upload single files, one by one instead of using a batch
process. In the image below:

- `Image Upload` to upload single image/raster
- `Table upload` to upload a vector (shapefile), you need to select all the 
shp standard files (.shp, .shx, .dbf, .prj).
- `Image collection` to upload several images as part of a collection (e.g. 
timeseries, multiband/timeseries).
- `Folder` to create a folder in your personal path/directory.

![](images/UploadCodeEditor.png)

Using the `Table upload` is the right option to upload vector data as shapefiles.

Datasets in GEE are called assets and to upload into assets we need to 
create the folder structure as needed.

All the datasets in GEE are called assets: `Images, Image Collections, and Tables(shp files)`.

To start working with assets from the command line we need to use the command line (CLI) 
`earthengine` (available as part of the GEE API installation).  

### 4. Authentication to Google Cloud

These steps will let you authenticate into Google Cloud. We need this as a 
staging platform between local drive and GEE.


```{bash} 
# ******************** Command Line Instructions ******************************#
# This command will try either to open a browser or display a link that you can
# paste into browser window.
gcloud auth login

# Set project ID as created at https://console.cloud.google.com/storage/
gcloud config set project xxxx-yyyy-172116

# Create a bucket, a folder for data staging
gsutil mb gs://my_bucket

# list buckets you have in your project
gsutil ls gs://

```

### 5. Uploading files from local disk into Google Cloud

A single copy can be made from the command line once you are authenticated 
into Google Cloud.

```{bash}
# ******************** Command Line Instructions ******************************#
gsutil -m cp *.tif gs://my_bucket/folder_A/

```
This will launch one task per file and will run in parallel by using available
cores. 

### 6. Using `earthengine` command to create folders/image collections

Before moving data into GEE you should create a target folder/image collection either 
using the `earthengine` CLI or the GEE Javascript code editor https://code.earthengine.google.com
The following bash block shows an example of how to use the CLI `earthengine` tool
to create a folder and an image collection.

```{bash}
# ******************** Command Line Instructions ******************************#
# Here we create a folder in GEE using earthengine tool
earthengine create folder users/gponce/usda_ars/shapefiles/AZ_LAND
# Create a collection (it looks like another folder, but internally it will be
# an image collection).
earthengine create collection users/my_username/collection_id

```
### 7. Transfer files from GCS bucket into GEE Asset

The following script can be saved as a bash script (transferGC2GE.sh) and 
run it as:
`./transferGC2GE.sh name_bucket_gcloud name_asset`

where:
- name_bucket_gcloud : name of the bucket created in Google Cloud 
- name_asset: relative path in GEE. E.g. if you have path like 
`users/my_user/FolderA` you should only include `my_user/FolderA` as the 
second parameter.


```{bash}
# ******************** Bash script ********************************************#
#!/usr/bin/env bash
# Usage:
# ./transfer_to_gee.sh src_bucket dest_asset

result=`earthengine create collection users/$2`
if `test -z "$result"`; then
    echo $result
    exit 1
fi
# In the following loop we get the entire path to all the geotifs using the specified 
# Gcloud bucket. Each file will have a format like this: gs://my_gee_bucket/FILE_January2000.tif
# Each call to earthengine it will launch a task that you can monitor in the JS Code editor 
# at the "tasks" tab.
for geotiff in `gsutil ls gs://$1/*.tif`; do  
    filename=`basename $geotiff`
    asset_id="${filename%.*}"
    earthengine upload image --asset_id=users/$2/$asset_id $geotiff
done

```
**Note:** The previous script will only move files into a collection without 
passing properties such as timestamp, which is very important once that you 
need to filter by date.
The following section shows one approach to attach timestamp and some other 
properties to each file uploaded into a collection.

### 8. Transfer files from GCS into GEE adding properties to the files

To accomplish this, the files to be uploaded must have information embedded in the 
filename, such as the date. 

Save this script as a bash script and put it in the folder where you have all the 
files that were transferred.
```{bash}
# ******************** Bash script ********************************************#
#!/bin/bash
# Define static variables
gcBucket="bucket_climate/monthly/tmean/"
imgCol="users/gponce/usda_ars/image_collections/climate/tmean/"
# Create asset in GEE
earthengine create collection $imgCol
strProv="(string)provider=USDA-ARS-SWRC"
# Get file names to extract date and call ingestion command for each file to be added into an asset as image collection
# Example of filenames used here are from a monthly timeseries: rainfall_US_20140101.tif
for file in *.tif; do
    var=$file                       # Get the file name
    yr=${var:12:4}                  # Extract year from the filename, to get this index use the filename and check the index for the year within the filename.  0-based-index
    mon=${var:16:2}                 # Same for extracting month
    vdate=$yr"-"$mon"-01T12:00:00"  # Concatenate strings to get a valid date format to use in the data ingestion co:mmand
    # Remove filename extension ".tif"
    name=$(echo $file | cut -f 1 -d '.')
    asset=$imgCol$name
    # For testing before running, use the following echo statament to print out the final command
    #echo earthengine upload image --asset_id="${asset}" --pyramiding_policy=sample --time_start="${vdate}" --property="\x22${strProv}\x22" --nodata_value=-9999 gs://$gcBucket$file
    # Call the ingestion command with the corresponding parameters to populate properties at each image ingested
    earthengine upload image --asset_id="${asset}" --pyramiding_policy=sample --time_start="${vdate}" --property="${strProv}" --nodata_value=-9999 gs://$gcBucket$file
done

```
Once the files are uploaded into GEE you can find the image collection at the path
you specified above or you can go use to JS code editor and type something like:
`var my_image_collecton = ee.ImageCollection('users/my_user/climate/image_collections/tmean')`
`print (my_image_collection)`  

### 9. Uploading NEON RGB-10cm images

**Example of how to upload the RGB Camera 10cm images**

***Santa Rita Experimental Range***

Once the RGB-10cm image files from NEON server are transferred into local server the 
uploading process can be started sending single images into Google Cloud as follows. 


```{bash}
# ******************** Command Line Instructions ******************************#
# Move to folder with images 
cd ~/DATA/NEON_RGB_10cm/
# Run gsutil to copy files into GCS bucket
gsutil -m cp *.tif gs://gee_neon/

```
That instruction will transfer all the tif files from local drive into GCS bucket.

**Setup GEE Image collection for RGB-10cm**

**Note:** Save the following bash block in a file and put it in the same folder 
as the RGB images.

The following block loop through tif files in the local server to get the file 
names. Then, those filenames are passed to the  `earthengine` tool to grab each file
from the GCS bucket and transfer it to GEE as an asset, each task started with this 
process can be monitored in the GEE-JS code editor in the tab `tasks`.

```{bash}
# ******************** Bash script ********************************************#
#!/bin/bash
imgFolder="users/gponce/usda_ars/image_collections/neon_srer_2017_rgb"
# First time, create collection
earthengine create collection $imgFolder
# Bucket in GCS
gcBucket=“gee_neon“
# Create asset in GEE
for file in *.tif; do
    # Remove .tif 
    name=$(echo $file | cut -f 1 -d '.')
    asset=$imgFolder$name
    # For testing before running, use the following echo statament to print out the final command
    #echo earthengine upload image --asset_id="${asset}" --pyramiding_policy=sample gs://$gcBucket$file
    # Call the ingestion command with the corresponding parameters to populate properties at each image ingested
    earthengine upload image --asset_id="${asset}" --pyramiding_policy=sample gs://$gcBucket$file
done

```
Once this process is done, the RGB-10cm are available as part of an ImageCollection
that can be found in the tab `Assets` in the JS Code editor.

### 10. Uploading NEON Hyperspectral

Hyperspectral data from NEON is delivered in HDF5 format.  To upload data into GEE
is required to convert these files into GeoTif format. 

These are the general steps to move data from local drive into GEE.

1. Extract bands (426) from each one of the flight-line files (.h5) and save as 
geotif (check storage before running this step) 
2. Merge bands back (tif) into single multi-band GeoTif 
3. Upload each multi-band geotif into GEE (Local --> GCS --> GEE) <br><br>

#### 10.1 Extract bands (426) from each one of the flight-line files (.h5).
```{r}
# ******************** R script ***********************************************#
# Load R-libraries
library(raster)
library(rhdf5)
library(rgdal)
library(maps)
library(unixtools)

# NOTES on rhdf5
# This installation worked:

# if (!requireNamespace("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")
# BiocManager::install("rhdf5", version = "3.8")

# Setup env. variables
rasterOptions(format = 'GTiff', overwrite = TRUE, 
              tmpdir = '~/REMOTE_SENSING/NEON/R_TMP/', 
              maxmemory=1e+9, chunksize=1e+5, datatype='INT2U', progress='text')

# --------- Global Variables --------------------------
# f <- 'NEON_D14_SRER_DP1_20170829_172625_reflectance.h5'
#args <- commandArgs(trailingOnly = TRUE)
#file <- 'NEON_D14_SRER_DP1_20170829_190359_reflectance.h5' #args[1]

# The following variables contain the structure of the H5 format, you can use
# HDFView software to make sure the structure of the h5 files remains the same
# and also to change it to the right site name, e.g. "SRER".
csr <- "/SRER/Reflectance/Metadata/Coordinate_System/"
spInf <- "/SRER/Reflectance/Metadata/"
sr <- "/SRER/"
reflAttr <- '/SRER/Reflectance/Reflectance_Data'

# --------- Functions ---------------------------------
ConvertBand2Raster <- function(band, f){
  # Converts single band into a tif
  #
  # Args: 
  #  band: numeric value with the band to process
  #  f: string with the hdf filename
  # 
  # Returns: 
  #  a matrix containing the reflectance data for the specific    band
  
  
  mapInfo <- h5read(f,paste0(csr,"Map_Info"))    # csr is global var.
  mapInfo <- noquote(unlist(strsplit(mapInfo,",")))
  
  my_crs <- paste0('+init=epsg:',h5read(f,paste0(csr,"EPSG Code")))
  my_res <- as.numeric(mapInfo[2])
  
  xMin <- as.numeric(mapInfo[4])
  yMax <- as.numeric(mapInfo[5])
  
  reflInfo <- h5readAttributes(f, reflAttr)  # reflAttr is a global variable
  
  # R get attributes for the Reflectance dataset
  nRows <- reflInfo$Dimensions[1]  #reflInfo$row_col_band[1]
  nCols <- reflInfo$Dimensions[2]  #reflInfo$row_col_band[2]
  nBands <- reflInfo$Dimensions[3]  #reflInfo$row_col_band[3]
  scaleFactor <- reflInfo$Scale_Factor
  
  #grab the no data value
  myNoDataValue <- reflInfo$Data_Ignore_Value
  
  
  H5close()
  # Read single band into a array
  out<- h5read(f,reflAttr,index=list(band, 1:nCols,1:nRows))
  
  # Convert from array to matrix
  out <- (out[1,,])
  
  # Transpose data, by using SRER-2017 H5 files this is needed
  out <-t(out)
  
  # Set no-data value 
  out[out == myNoDataValue] <- NA
  
  # Turn matrix into a raster with crs
  outr <- raster(out,crs=my_crs)
  
  # Set variables with extent for the raster based on the Reflectance_Data attributes
  xMin <- reflInfo$Spatial_Extent_meters[1]
  xMax <- reflInfo$Spatial_Extent_meters[2]
  yMin <- reflInfo$Spatial_Extent_meters[3]
  yMax <- reflInfo$Spatial_Extent_meters[4]
  
  # Create extent
  rasExt  <- extent(xMin,xMax,yMin,yMax)
  
  # Set extent to raster
  extent(outr) <- rasExt
  H5close()
  # Return raster object
  
  return(outr)
}


# Set writing and files location
#writePath <- "~/REMOTE_SENSING/NEON/SRER/CAMPAIGN_2017/SPECTROMETER/REFLECTANCE/NEON_TMP/"
# Get all the h5 file name references into a list
# Get the full path
files <- list.files(path="/fullPath/NEON_WGEW_2018/L3/Spectrometer/Reflectance", pattern="*.h5", full.names=T, recursive=FALSE)
# Get the index from the filename e.g: For the file /fullPath/NEON_D14_WGEW_DP3_608000_3515000.h5 
p <- files[1]
# This index is needed to create the tif files 
f_index <- unlist(gregexpr("NEON_D",p))[1]



# Loop through each h5 file and extract and save its bands as GeoTif 
#setwd('~/DATA/NEON_DATA/REFLECTANCE/')
for (fi in files) {
  v_bands <- list(1:426)
  print(paste0("Processing file ", fi))
  ptm <- proc.time()
  v_rast <- lapply(1:426, ConvertBand2Raster, f = fi)
  proc.time() - ptm
  v_stack <- stack(v_rast)
  v_band_names <- paste("Band_", unlist(v_bands),sep="")
  names(v_stack) <- v_band_names
  
  for (i in unlist(v_bands)) {
    r_name <- paste0(substr(gsub(pattern="\\.h5","",fi),
                            f_index,nchar(fi)),
                     '_band_', sprintf("%03d",i),'.tif')
    print(paste0("Processing file ", r_name))
    writeRaster(v_stack[[i]], filename=r_name, format="GTiff")
  }
  removeTmpFiles(h=0)
  gc()
}

```
At this point there is a single GeoTif per band-file stored.

The next steps are performed in `bash` to organize the files from each band and 
prepare the multiband GeoTifs that will be ingested into GEE. <br><br>

#### 10.2 Merge bands back (tif) into single multi-band GeoTif 
```{bash createMerge}
# ******************** Command Line Instructions ******************************#
# Do this by date, the following is just an example of a single line and the 
# the file naming changes depending on the site, this case is from SRER site.
# e.g. 
ls NEON_D14_SRER_DP1_20170824_160546_reflectance_band_*.tif > btif.txt  
echo cat btif.txt > processh5.sh
vi processh5.sh

# This is the one-line instruction in Redhat command line, should work in any 
# linux flavor. Change "NEON_D14_SRER_DP3" section for the corresponding site
find -type f -name '*.tif' | awk 'BEGIN{FS="_"}{ print "NEON_D14_SRER_DP3_"$5"_"$6"_reflectance_band_*.tif"}' | sort | unique > btif.txt
echo cat btif.txt > processh5.sh

# The previous on-liner is what you need to run for all the files

# To make the previous command for several files open the file `processh5.sh 
# in `vi` and run the following commands:

# Insert `echo` starting at each row 
:%s /^/echo /g 
# Insert ` >> alltifs.txt` at the end of rows
%s /$/ >> alltifs.txt/g 

# Move to the first row of the file(processh5.sh) and change `>>` by this `>` to create instead of `>>` which is for appending.

# You should now see something like this in the file:  
echo NEON_D14_SRER_DP1_20170824_161655_reflectance_band_*.tif > alltifs.txt
echo NEON_D14_SRER_DP1_20170824_162407_reflectance_band_*.tif >> alltifs.txt
echo NEON_D14_SRER_DP1_20170824_163106_reflectance_band_*.tif >> alltifs.txt
echo NEON_D14_SRER_DP1_20170824_164049_reflectance_band_*.tif >> alltifs.txt
echo NEON_D14_SRER_DP1_20170824_164453_reflectance_band_*.tif >> alltifs.txt
...

# and so on, for each FILE-DATE…
# Close and Run the file `processh5.sh`
chmod +x processh5.sh
./processh5.sh

# After running `processh5.sh` you should get `alltifs.txt` file, then join all 
# the filenames into a single line.

# Using `awk` ;-) 
cat alltifs.txt | awk '{print}' ORS=' ' > all_bands_files

# The `all_bands_files` is used with gdal tools to create mosaics

```

In the following block, GDAL Tools (gdalbuildvrt) are used to create a virtual file
and then generate the final multiband file.

```{bash mergeBands}
# The goal of this script is to work with GDAL tools `gdalbuildvrt` and 
# `gdal_translate`. With these tools is possible to create a multi-band file.  
# 
# ******************** Command Line Instructions ******************************#
# Insert the following line in the bash file. Where `all_bands_files` is a one line
# text file with all the file names of the tif files for each band in a single line 
# for each date it is required to get one call to `gdalbuildvrt.

# Again, this is just a single example and you don't have to run this...
gdalbuildvrt -separate NEON_WGEW_2018.vrt NEON_FILE_NAME_DATE.tif  all_bands_files

# For SRER 2018 these one-liners were used
# This is used to build the calls to `gdalbuildvrt`
# This one-liner can read as: List all the 'tif' files, then subtract a portion 
# of the filename to create the .vrt file, then pre-append to each .vrt file the 
# instruction `gdalbuildvrt -separate` and send the results into 
# `gdalbuild_first_part.txt`  Don't forget to change the file name... 
find -type f -name '*.tif' | awk 'BEGIN{FS="_"}{ print "NEON_D14_SRER_DP3_"$5"_"$6"_reflectance_band_*.tif"}' | sort | uniq | awk '{print "gdalbuildvrt -separate "substr($0,0,32)".vrt "}' > gdalbuild_first_part.txt

# The output should have as many lines as the number of `.h5` files.  
# Something like: gdalbuildvrt -separate NEON_D14_SRER_DP3_608000_3515000.vrt

# Now, create an index to make a join later
awk '{printf ("%.3d %s\n", NR, $0) }' alltifs.txt > alltifs2.txt

# Create another index
awk '{printf ("%.3d %s\n", NR, $0) }' gdalbuild_first_part.txt  > gdal_temp.txt

# Create one-liner for gdalbuildvrt, this is done to achieve the full one-line 
# gdalbuildvrt instruction.
join -1 1 -2 1 gdal_temp.txt alltifs2.txt > one-liner.sh

# Remove index column from the resulting join
# Since the joined column start like this: 001 NEON...
# The first 4 characters should be removed, therefore use this
sed 's/^.\{4\}//' one-liner.sh  > one-liner2.sh

# one-liner.sh will contain the full instruction to create all the vrt files
# Run one-liner2.sh to get `vrt` files
chmod +x one-liner2.sh
./one-liner2.sh

# The next command is to use the virtual files and generate the multi-band file
# the options were provided by the GEE data ingestion team.

# Repeat the following instruction as many times as different multi-bands will 
# be created. See the command line used to generate a file with this instruction
# on each `vrt` file.

gdal_translate --config GDAL_CACHEMAX 1024 -of GTiff -co COMPRESS=DEFLATE -co ZLEVEL=9 -co BIGTIFF=IF_SAFER -co INTERLEAVE=BAND -co NUM_THREADS=60 NEON_FILE_NAME_DATE.vrt NEON_D14_SRER_D...mb.tif

# A one-liner to create a "bash" for running over all the `vrt` files 
# could be something like this:
find ./ -maxdepth 1 -name "*.vrt"  | sed 's/\.vrt$//' | sed 's/\.\///' | awk '{print "gdal_translate --config GDAL_CACHEMAX 1024 -of GTiff -co COMPRESS=DEFLATE -co ZLEVEL=9 -co BIGTIFF=IF_SAFER -co INTERLEAVE=BAND -co NUM_THREADS=60 "$0".vrt "$0"_reflectance_mb.tif"}' > create_multi_band_files.sh

# Run the resulting file as bash script:
chmod +x create_multi_band_files.sh
./create_multi_band_files.sh

# At this point, you should have a list of `tif` files where each file should 
# have the 426 bands.  In total, for WGEW 2018 survey, 369 files (.h5) were produced.


```
The `gdal_translate` is the last step in creating multiband files as GeoTif. After
this step the output files are ready to be sent to Google Cloud Storage bucket and
then transferred into GEE. <br><br>

#### 10.3 Upload each multi-band geotif into GEE (Local --> GCS --> GEE)

Start by moving multiband files to GCS. Using the `gsutil` tool copy 
the multi-band geotif files into the GCS-bucket. 


```{bash upload2GCS}
# The bucket/folder structure below was created in the Google Cloud Console website

gsutil cp *mb.tif gs://gee_neon_srer_2018/L3/Spectrometer/Reflectances/

```
Once files are in the GCS bucket the following step is to move those files into
GEE as an image collection. For doing this, save the following bash script in a 
file and run it in unix where you are authenticated to google cloud and 
earthengine.

```{bash move2gee}

# ******************** Bash script ********************************************#
#!/bin/bash
imgFolder="users/gponce/usda_ars/image_collections/neon/srer_2018/neon_srer_2018_hs"
# First time, create collection
earthengine create collection $imgFolder
# Bucket in GCS
imgFolder=$imgFolder"/"
strProv="(string)provider=BATTELLE_NEON_SRER_2018"
gcBucket="gee_neon_srer_2018/L3/Spectrometer/Reflectances/"
vdate="2018-08-30T12:00:00"
# Create asset in GEE
for file in *_mb.tif; do
    # Remove .tif 
    name=$(echo $file | cut -f 1 -d '.')
    asset=$imgFolder$name
    # For testing before running, use the following echo statament to print out 
    # the final command.
    
    #echo earthengine upload image --asset_id="${asset}" --time_start="${vdate}" --property="${strProv}" --pyramiding_policy=sample gs://$gcBucket$file
    
    # Call the ingestion command with the corresponding parameters to populate properties at each image ingested
    earthengine upload image --asset_id="${asset}" --time_start="${vdate}" --property="${strProv}" --pyramiding_policy=sample gs://$gcBucket$file
done

```
This concludes the process for uploading spectrometer reflectances from NEON 
into Google Earth Engine. 
<br><br>
